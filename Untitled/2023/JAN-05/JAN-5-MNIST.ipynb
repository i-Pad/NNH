{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omg0207/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv, det\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "from torch import sigmoid, tanh\n",
    "from torch import Tensor, exp, log\n",
    "from torch.nn import Sequential\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.init import uniform_\n",
    "from torch.nn.functional import linear\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paper: Neural Arithmetic Logic Units (https://arxiv.org/pdf/1808.00508.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for addition and subtraction\n",
    "class ASCell(nn.Module):\n",
    "    def __init__(self, in_shape, out_shape):\n",
    "        super().__init__()\n",
    "        self.in_shape = in_shape\n",
    "        self.out_shape = out_shape\n",
    "\n",
    "        self.W_ = Parameter(Tensor(out_shape, in_shape))\n",
    "        self.M_ = Parameter(Tensor(out_shape, in_shape))\n",
    "\n",
    "        uniform_(self.W_, 0.0, 1.0), uniform_(self.M_, 0.0, 1.0)\n",
    "        self.register_parameter('bias', None)\n",
    "\n",
    "    # a = Wx\n",
    "    # W = tanh(W) * sigmoid(M)\n",
    "    # * is elementwise product\n",
    "    def forward(self, X):\n",
    "        #print('W:', self.W_.shape, 'X:', X.shape)\n",
    "        W = tanh(self.W_) * sigmoid(self.M_)\n",
    "\n",
    "        # linear: XW^T + b\n",
    "        return linear(X, W, self.bias)\n",
    "        #return torch.matmul(X, W.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for multiplication and division\n",
    "class MDCell(nn.Module):\n",
    "    def __init__(self, in_shape, out_shape):\n",
    "        super().__init__()\n",
    "        self.in_shape = in_shape\n",
    "        self.out_shape = out_shape\n",
    "\n",
    "#         self.G = Parameter(Tensor(out_shape, in_shape))\n",
    "        self.nac = ASCell(in_shape, out_shape)\n",
    "\n",
    "#         uniform_(self.G, 0.0, 1.0)\n",
    "        # epsilon prevents log0\n",
    "        self.eps = 1e-5\n",
    "        self.register_parameter('bias', None)\n",
    "\n",
    "    # y = g * a + (1 - g) * m\n",
    "    # m = exp W(log(|x| + e)), g = sigmoid(Gx)\n",
    "    # * is elementwise product\n",
    "    # a is from nac\n",
    "    \n",
    "    # y = exp W(log(|X| + e))\n",
    "    # W = tanh(W) * sigmoid(M)\n",
    "    # * is elementalwise product\n",
    "    def forward(self, X):\n",
    "#         a = self.nac(X)\n",
    "#         g = sigmoid(linear(X, self.G, self.bias))\n",
    "\n",
    "#         ag = g * a\n",
    "        log_in = log(abs(X) + self.eps)\n",
    "        m = exp(self.nac(log_in))\n",
    "#         md = (1 - g) * m\n",
    "\n",
    "#         return ag + md\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.MD1 = MDCell(in_shape=784, out_shape=64)\n",
    "        self.MD2 = MDCell(in_shape=64, out_shape=32)\n",
    "        self.MD3 = MDCell(in_shape=32, out_shape=16)\n",
    "        self.AS1 = ASCell(in_shape=64, out_shape=10)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.MD1(X)\n",
    "        X = self.AS1(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MNIST('/home/data/MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 60000\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = mnist.load_training()\n",
    "X_test, y_test = mnist.load_training()\n",
    "\n",
    "print(type(X_train[0]), len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NALU(input_shape=4, output_shape=1, n_layers=2, hidden_shape=2)\n",
    "model = Custom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_train = torch.from_numpy(X_train)\n",
    "y_train = torch.from_numpy(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8], device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x7f30b0828fd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TensorDataset(X_train, y_train)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_size = 128\n",
    "dataloader = DataLoader(dataset, batch_size=b_size, shuffle=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "cur_loss = 0.0\n",
    "epos = []\n",
    "los = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|          | 0/469 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Byte for argument #2 'target' in call to _thnn_nll_loss_forward",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-bec3017ee99b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2822\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2823\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2824\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Byte for argument #2 'target' in call to _thnn_nll_loss_forward"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    with tqdm(dataloader) as tepoch:\n",
    "        for batch_idx, samples in enumerate(tepoch):\n",
    "            tepoch.set_description(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            X_t, y_t = samples\n",
    "            \n",
    "            pred = model(X_t)\n",
    "            \n",
    "            cost = F.cross_entropy(pred, y_t)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            tepoch.set_postfix(loss=cost.item())\n",
    "            cur_loss = cost.item()\n",
    "            \n",
    "#         with torch.no_grad():\n",
    "#             y_test = model(X_test)\n",
    "#             print(f\"End of epoch {epoch + 1}\")\n",
    "#             print_loss_accuracy(y_test_log_pred, y_test, loss_function)\n",
    "#             print(\"---\")\n",
    "            \n",
    "        epos.append(epoch + 1)\n",
    "        los.append(cur_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialization 에 따라 수렴 여부가 결정됨 -> 항상 수렴시킬 방법?\n",
    "\n",
    "-> initialization 할 때 exclude negative number\n",
    "\n",
    "-> 이래도 잘 안 됨. G 학습이 어려워서 그럴지도? G 를 학습시키지 말고 덧셈과 곱셈을 분리해버리면 어떨까?\n",
    "\n",
    "-> 훨씬 안정적임. G 가 문제였던 것 같음!!\n",
    "\n",
    "<!-- 음수가 포함되면 나누기 연산이 들어갈 확률이 높아져서 그런 것 같음 -->\n",
    "\n",
    "필요한 연산보다 더 많은 layer 를 만들면 학습이 되나?\n",
    "\n",
    "-> 되기는 하지만 타이트할 때보다 잘 되지는 않음\n",
    "\n",
    "그럼 이제 3 x 3 이랑 4 x 4 도전\n",
    "\n",
    "-> 3 x 3 은 성공 16023.6797\n",
    "\n",
    "-> 4 x 4 는 좀 어려운듯? 493209.6875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn = 2\n",
    "# i = 4\n",
    "# o = 1\n",
    "# h = 2\n",
    "# for n in range(nn):\n",
    "#     f1 = h if n > 0 else i\n",
    "#     f2 = h if n < nn - 1 else o\n",
    "    \n",
    "#     print('({}, {})'.format(f1, f2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epos, los, label='train')\n",
    "plt.title('model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "# plt.savefig('aaa.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(epos, los, label='train')\n",
    "# plt.title('model loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('loss')\n",
    "# plt.legend()\n",
    "# plt.savefig('aaa.png')\n",
    "# plt.show()\n",
    "\n",
    "# torch.save(model, 'aaa.pt')\n",
    "\n",
    "# nn = 2\n",
    "# i = 4\n",
    "# o = 1\n",
    "# h = 2\n",
    "# for n in range(nn):\n",
    "#     f1 = h if n > 0 else i\n",
    "#     f2 = h if n < nn - 1 else o\n",
    "    \n",
    "#     print('({}, {})'.format(f1, f2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
