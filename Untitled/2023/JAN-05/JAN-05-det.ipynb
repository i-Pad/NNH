{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv, det\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "from torch import sigmoid, tanh\n",
    "from torch import Tensor, exp, log\n",
    "from torch.nn import Sequential\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.init import uniform_\n",
    "from torch.nn.functional import linear\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paper: Neural Arithmetic Logic Units (https://arxiv.org/pdf/1808.00508.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NacCell(nn.Module):\n",
    "#     def __init__(self, in_shape, out_shape):\n",
    "#         super().__init__()\n",
    "#         self.in_shape = in_shape\n",
    "#         self.out_shape = out_shape\n",
    "\n",
    "#         self.W_ = Parameter(Tensor(out_shape, in_shape))\n",
    "#         self.M_ = Parameter(Tensor(out_shape, in_shape)\n",
    "\n",
    "#         uniform_(self.W_, 0.0, 1.0), uniform_(self.M_, 0.0, 1.0)\n",
    "#         self.register_parameter('bias', None)\n",
    "\n",
    "#     # a = Wx\n",
    "#     # W = tanh(W) * sigmoid(M)\n",
    "#     # * is elementwise product\n",
    "#     def forward(self, X):\n",
    "#         #print('W:', self.W_.shape, 'X:', X.shape)\n",
    "#         W = tanh(self.W_) * sigmoid(self.M_)\n",
    "\n",
    "#         # linear: XW^T + b\n",
    "#         return linear(X, W, self.bias)\n",
    "#         #return torch.matmul(X, W.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for addition and subtraction\n",
    "class ASCell(nn.Module):\n",
    "    def __init__(self, in_shape, out_shape):\n",
    "        super().__init__()\n",
    "        self.in_shape = in_shape\n",
    "        self.out_shape = out_shape\n",
    "\n",
    "        self.W_ = Parameter(Tensor(out_shape, in_shape))\n",
    "        self.M_ = Parameter(Tensor(out_shape, in_shape))\n",
    "\n",
    "        uniform_(self.W_, 0.0, 1.0), uniform_(self.M_, 0.0, 1.0)\n",
    "        self.register_parameter('bias', None)\n",
    "\n",
    "    # a = Wx\n",
    "    # W = tanh(W) * sigmoid(M)\n",
    "    # * is elementwise product\n",
    "    def forward(self, X):\n",
    "        #print('W:', self.W_.shape, 'X:', X.shape)\n",
    "        W = tanh(self.W_) * sigmoid(self.M_)\n",
    "\n",
    "        # linear: XW^T + b\n",
    "        return linear(X, W, self.bias)\n",
    "        #return torch.matmul(X, W.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NaluCell(nn.Module):\n",
    "#     def __init__(self, in_shape, out_shape):\n",
    "#         super().__init__()\n",
    "#         self.in_shape = in_shape\n",
    "#         self.out_shape = out_shape\n",
    "\n",
    "#         self.G = Parameter(Tensor(out_shape, in_shape))\n",
    "#         self.nac = NacCell(in_shape, out_shape)\n",
    "\n",
    "#         uniform_(self.G, 0.0, 1.0)\n",
    "#         # epsilon prevents log0\n",
    "#         self.eps = 1e-5\n",
    "#         self.register_parameter('bias', None)\n",
    "\n",
    "#     # y = g * a + (1 - g) * m\n",
    "#     # m = exp W(log(|x| + e)), g = sigmoid(Gx)\n",
    "#     # * is elementwise product\n",
    "#     # a is from nac\n",
    "#     def forward(self, X):\n",
    "#         a = self.nac(X)\n",
    "#         g = sigmoid(linear(X, self.G, self.bias))\n",
    "\n",
    "#         ag = g * a\n",
    "#         log_in = log(abs(X) + self.eps)\n",
    "#         m = exp(self.nac(log_in))\n",
    "#         md = (1 - g) * m\n",
    "\n",
    "#         return ag + md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for multiplication and division\n",
    "class MDCell(nn.Module):\n",
    "    def __init__(self, in_shape, out_shape):\n",
    "        super().__init__()\n",
    "        self.in_shape = in_shape\n",
    "        self.out_shape = out_shape\n",
    "\n",
    "#         self.G = Parameter(Tensor(out_shape, in_shape))\n",
    "        self.nac = ASCell(in_shape, out_shape)\n",
    "\n",
    "#         uniform_(self.G, 0.0, 1.0)\n",
    "        # epsilon prevents log0\n",
    "        self.eps = 1e-5\n",
    "        self.register_parameter('bias', None)\n",
    "\n",
    "    # y = g * a + (1 - g) * m\n",
    "    # m = exp W(log(|x| + e)), g = sigmoid(Gx)\n",
    "    # * is elementwise product\n",
    "    # a is from nac\n",
    "    \n",
    "    # y = exp W(log(|X| + e))\n",
    "    # W = tanh(W) * sigmoid(M)\n",
    "    # * is elementalwise product\n",
    "    def forward(self, X):\n",
    "#         a = self.nac(X)\n",
    "#         g = sigmoid(linear(X, self.G, self.bias))\n",
    "\n",
    "#         ag = g * a\n",
    "        log_in = log(abs(X) + self.eps)\n",
    "        m = exp(self.nac(log_in))\n",
    "#         md = (1 - g) * m\n",
    "\n",
    "#         return ag + md\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NaluLayer(nn.Module):\n",
    "#     def __init__(self, input_shape, output_shape, n_layers, hidden_shape):\n",
    "#         super().__init__()\n",
    "#         self.input_shape = input_shape\n",
    "#         self.output_shape = output_shape\n",
    "#         self.n_layers = n_layers\n",
    "#         self.hidden_shape = hidden_shape\n",
    "        \n",
    "#         layers = [NaluCell(hidden_shape if n > 0 else input_shape, hidden_shape if n < n_layers - 1 else output_shape) for n in range(n_layers)]\n",
    "#         self.model = Sequential(*layers)\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NALU(nn.Module):\n",
    "#     def __init__(self, input_shape, output_shape, n_layers, hidden_shape):\n",
    "#         super().__init__()\n",
    "#         self.nalu1 = NaluLayer(input_shape, output_shape, n_layers, hidden_shape)\n",
    "\n",
    "#         print('input: {}, output: {}, n: {}, hidden: {}'.format(input_shape, output_shape, n_layers, hidden_shape))\n",
    "        \n",
    "#     def forward(self, X):\n",
    "#         X = self.nalu1(X)\n",
    "#         X = X.squeeze()\n",
    "\n",
    "#         return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NALU2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.MD1 = MDCell(in_shape=4, out_shape=2)\n",
    "        self.AS1 = ASCell(in_shape=2, out_shape=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.MD1(X)\n",
    "        X = self.AS1(X)\n",
    "        X = X.squeeze()\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 1\n",
    "n_train = 100000\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "while num <= n_train:\n",
    "    A = np.random.randint(1, 50, size=(2, 2))\n",
    "    \n",
    "    if det(A) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        A = A.astype(np.float32)\n",
    "        X_train.append(A)\n",
    "        y_train.append(det(A))\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_train = torch.from_numpy(X_train)\n",
    "X_train = X_train.reshape(n_train, 4)\n",
    "y_train = torch.from_numpy(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 1\n",
    "n_val = 10000\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "while num <= n_val:\n",
    "    A = np.random.randint(1, 50, size=(2, 2))\n",
    "    \n",
    "    if det(A) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        A = A.astype(np.float32)\n",
    "        X_val.append(A)\n",
    "        y_val.append(det(A))\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "X_val = torch.from_numpy(X_val)\n",
    "X_val = X_val.reshape(n_val, 4)\n",
    "y_val = torch.from_numpy(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_val.shape)\n",
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(sum(abs(y_train)) / n_train)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NALU(input_shape=4, output_shape=1, n_layers=2, hidden_shape=2)\n",
    "model = NALU2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "  print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "y_val = y_val.to(device)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X_train, y_train)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_size = 128\n",
    "dataloader = DataLoader(dataset, batch_size=b_size, shuffle=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "train_loss = 0.0\n",
    "val_loss = 0.0\n",
    "b_idx = 0\n",
    "epos = []\n",
    "los = []\n",
    "v_los = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_train / b_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    with tqdm(dataloader) as tepoch:\n",
    "        tepoch.set_description(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        b_idx = 0\n",
    "        \n",
    "        for batch_idx, samples in enumerate(tepoch):\n",
    "            b_idx += 1\n",
    "            X_t, y_t = samples\n",
    "            \n",
    "            pred = model(X_t)\n",
    "            cost = F.mse_loss(pred, y_t)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss = cost.item()\n",
    "            train_loss += loss\n",
    "            \n",
    "            if b_idx < 782:\n",
    "                tepoch.set_postfix({'Train loss(in progress)': loss})\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    p_val = model(X_val)\n",
    "                    val_cost = F.mse_loss(p_val, y_val)\n",
    "                    val_loss = val_cost.item()\n",
    "                \n",
    "                train_loss /= b_idx \n",
    "#                 print('train:', train_loss, 'val:', val_loss, 'b:', b_idx)\n",
    "                tepoch.set_postfix({'Train loss(final)': train_loss, 'Val loss': val_loss})\n",
    "#                 tepoch.close()\n",
    "        \n",
    "        epos.append(epoch + 1)\n",
    "        los.append(train_loss)\n",
    "        v_los.append(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialization 에 따라 수렴 여부가 결정됨 -> 항상 수렴시킬 방법?\n",
    "\n",
    "-> initialization 할 때 exclude negative number\n",
    "\n",
    "-> 이래도 잘 안 됨. G 학습이 어려워서 그럴지도? G 를 학습시키지 말고 덧셈과 곱셈을 분리해버리면 어떨까?\n",
    "\n",
    "-> 훨씬 안정적임. G 가 문제였던 것 같음!!\n",
    "\n",
    "<!-- 음수가 포함되면 나누기 연산이 들어갈 확률이 높아져서 그런 것 같음 -->\n",
    "\n",
    "필요한 연산보다 더 많은 layer 를 만들면 학습이 되나?\n",
    "\n",
    "-> 되기는 하지만 타이트할 때보다 잘 되지는 않음\n",
    "\n",
    "그럼 이제 3 x 3 이랑 4 x 4 도전\n",
    "\n",
    "-> 3 x 3 은 성공 16023.6797\n",
    "\n",
    "-> 4 x 4 는 좀 어려운듯? 493209.6875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "  print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn = 2\n",
    "# i = 4\n",
    "# o = 1\n",
    "# h = 2\n",
    "# for n in range(nn):\n",
    "#     f1 = h if n > 0 else i\n",
    "#     f2 = h if n < nn - 1 else o\n",
    "    \n",
    "#     print('({}, {})'.format(f1, f2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epos, los, label='train')\n",
    "plt.plot(epos, v_los, label='validation')\n",
    "plt.title('model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "# plt.savefig('aaa.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "\n",
    "ax1.plot(epos, los, label='train')\n",
    "ax2.plot(epos, v_los, label='validation')\n",
    "\n",
    "ax1.set_ylim(4e+5, 5e+5)\n",
    "ax2.set_ylim(0, 100)\n",
    "\n",
    "ax1.spines['bottom'].set_visible(False)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax1.xaxis.tick_top()\n",
    "ax1.tick_params(labeltop=False)\n",
    "ax2.xaxis.tick_bottom()\n",
    "\n",
    "kwargs = dict(mark=[(-1, -0.5), (1, 0.5)], markersize=12, linestyle='none', color='k', mec='k', mew=1, clip_on=False)\n",
    "ax1.plot([0, 1], [0, 0], transform=ax1.transAxes, **kwargs)\n",
    "ax2.plot([0, 1], [1, 1], transform=ax2.transAxes, **kwargs)\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(epos, los, label='train')\n",
    "# plt.title('model loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('loss')\n",
    "# plt.legend()\n",
    "# plt.savefig('aaa.png')\n",
    "# plt.show()\n",
    "\n",
    "# torch.save(model, 'aaa.pt')\n",
    "\n",
    "# nn = 2\n",
    "# i = 4\n",
    "# o = 1\n",
    "# h = 2\n",
    "# for n in range(nn):\n",
    "#     f1 = h if n > 0 else i\n",
    "#     f2 = h if n < nn - 1 else o\n",
    "    \n",
    "#     print('({}, {})'.format(f1, f2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
